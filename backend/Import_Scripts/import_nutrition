import pandas as pd
import os
from dotenv import load_dotenv
import weaviate
from weaviate.classes.init import Auth
from weaviate.classes.config import Property, DataType, Configure

load_dotenv() # Load Keys from env
weaviate_url = os.environ["WEAVIATE_URL"]
weaviate_api_key = os.environ["WEAVIATE_API_KEY"]

client = weaviate.connect_to_weaviate_cloud( #Connect to weaviate cloud database
    cluster_url=weaviate_url,
    auth_credentials=Auth.api_key(weaviate_api_key), # Set API
)

assert client.is_ready(), "Weaviate connection failed."

df = pd.read_csv("data/nutrition.csv") # Read the nutrition data CSV file

if df.columns[0].startswith("Unnamed"): # Check if the first column is unnamed and drop it
    df = df.drop(columns=[df.columns[0]])

numerical_cols = set(df.select_dtypes(include=["int64", "float64"]).columns) #Seperate numerical columns from text columns
text_cols = set(df.columns) - numerical_cols

df = df.fillna("") # Fill NaN values with empty strings for text columns

# Define descriptions for each column
#These descriptions are used to provide more context to the LLM about what that property means so that it can generate more accurate results
descriptions = {
    "name": "Name of the food item",
    "serving_size": "Standard serving size of the food item",
    "calories": "Total calories in the serving size",
    "total_fat": "Total fat content (g)",
    "saturated_fat": "Saturated fat content (g)",
    "cholesterol": "Cholesterol content (mg)",
    "sodium": "Sodium content (mg)",
    "protein": "Protein content (g)",
    "carbohydrate": "Total carbohydrate content (g)",
    "fiber": "Dietary fiber content (g)",
    "sugars": "Total sugars content (g)",
    "vitamin_c": "Vitamin C content (mg)",
    "vitamin_d": "Vitamin D content (µg)",
    "calcium": "Calcium content (mg)",
    "iron": "Iron content (mg)",
    "potassium": "Potassium content (mg)",
    "zink": "Zinc content (mg)",
    "water": "Water content (g)",
}

collection_name = "FoodNutrition" #Name the collection
if client.collections.exists(collection_name): #If it already exists replace it
    client.collections.delete(collection_name)

properties = []
for col in df.columns: 
    dtype = DataType.NUMBER if col in numerical_cols else DataType.TEXT #Determine if the column is numeric or text and assign the appropriate data type
    prop = Property(
        name=col,
        data_type=dtype,
        description=descriptions.get(col, col.replace("_", " ").capitalize()) #Add the name, description, datatype and prettify it to the property
    )
    properties.append(prop)

client.collections.create( #create the collection with the properties
    name=collection_name,
    properties=properties,
    vectorizer_config=Configure.Vectorizer.text2vec_weaviate(), #Convert the textual data into vectors using weviate's embedding model
    generative_config=Configure.Generative.google( #Configure the generative model to use Google's Gemini for searching
        project_id=os.environ["GOOGLE_PROJECT_ID"],
        model_id="gemini-2.0-flash-001" 
    )
)
collection = client.collections.get(collection_name)

with collection.batch.fixed_size(batch_size=100) as batch: #Start a fixed-size batch upload 
    for _, row in df.iterrows():
        data_obj = row.to_dict() #Make each row a dictionary
        batch.add_object(data_obj) #Add the object to the batch to be uploaded
        if batch.number_errors > 10: 
            print("Too many errors — stopping upload.")
            break

if collection.batch.failed_objects:
    print("Failed objects:", len(collection.batch.failed_objects))
else:
    print("Upload complete!")

client.close()
